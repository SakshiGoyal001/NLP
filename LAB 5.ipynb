{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aah</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>absentminded</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absurd</th>\n",
       "      <th>abuse</th>\n",
       "      <th>academy</th>\n",
       "      <th>...</th>\n",
       "      <th>youyou</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombielike</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>àshfá</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ELEMENTAL (2023)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE MONKEY KING (2023)</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DESPICABLE ME 3 (2017)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLAUS (2019)</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE BOSS BABY: FAMILY BUSINESS (2021)</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       aah  abandoned  able  about  above  \\\n",
       "full_name                                                                   \n",
       "ELEMENTAL (2023)                         0          0     1     11      1   \n",
       "THE MONKEY KING (2023)                   4          0     2     21      0   \n",
       "DESPICABLE ME 3 (2017)                   0          0     0      0      0   \n",
       "KLAUS (2019)                             8          0     0     38      0   \n",
       "THE BOSS BABY: FAMILY BUSINESS (2021)    0          2     0     13      1   \n",
       "\n",
       "                                       absentminded  absolutely  absurd  \\\n",
       "full_name                                                                 \n",
       "ELEMENTAL (2023)                                  0           0       0   \n",
       "THE MONKEY KING (2023)                            0           0       0   \n",
       "DESPICABLE ME 3 (2017)                            0           0       0   \n",
       "KLAUS (2019)                                      1           2       0   \n",
       "THE BOSS BABY: FAMILY BUSINESS (2021)             0           4       1   \n",
       "\n",
       "                                       abuse  academy  ...  youyou  yum  \\\n",
       "full_name                                              ...                \n",
       "ELEMENTAL (2023)                           0        1  ...       0    1   \n",
       "THE MONKEY KING (2023)                     0        0  ...       0    0   \n",
       "DESPICABLE ME 3 (2017)                     0        0  ...       0    0   \n",
       "KLAUS (2019)                               1        0  ...       2    0   \n",
       "THE BOSS BABY: FAMILY BUSINESS (2021)      0        0  ...       0    6   \n",
       "\n",
       "                                       yummy  yup  zero  zip  zombielike  \\\n",
       "full_name                                                                  \n",
       "ELEMENTAL (2023)                           0    1     1    0           0   \n",
       "THE MONKEY KING (2023)                     0    3     0    0           0   \n",
       "DESPICABLE ME 3 (2017)                     0    0     0    0           0   \n",
       "KLAUS (2019)                               0    0     0    0           3   \n",
       "THE BOSS BABY: FAMILY BUSINESS (2021)      2    0     0    1           0   \n",
       "\n",
       "                                       zombies  zone  àshfá  \n",
       "full_name                                                    \n",
       "ELEMENTAL (2023)                             0     0      0  \n",
       "THE MONKEY KING (2023)                       0     0     10  \n",
       "DESPICABLE ME 3 (2017)                       0     0      0  \n",
       "KLAUS (2019)                                 2     1      0  \n",
       "THE BOSS BABY: FAMILY BUSINESS (2021)        0     0      0  \n",
       "\n",
       "[5 rows x 4167 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>full_name</th>\n",
       "      <th>ELEMENTAL (2023)</th>\n",
       "      <th>THE MONKEY KING (2023)</th>\n",
       "      <th>DESPICABLE ME 3 (2017)</th>\n",
       "      <th>KLAUS (2019)</th>\n",
       "      <th>THE BOSS BABY: FAMILY BUSINESS (2021)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aah</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>above</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "full_name  ELEMENTAL (2023)   THE MONKEY KING (2023)  DESPICABLE ME 3 (2017)  \\\n",
       "aah                        0                       4                       0   \n",
       "abandoned                  0                       0                       0   \n",
       "able                       1                       2                       0   \n",
       "about                     11                      21                       0   \n",
       "above                      1                       0                       0   \n",
       "\n",
       "full_name  KLAUS (2019)  THE BOSS BABY: FAMILY BUSINESS (2021)  \n",
       "aah                   8                                      0  \n",
       "abandoned             0                                      2  \n",
       "able                  0                                      0  \n",
       "about                38                                     13  \n",
       "above                 0                                      1  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "\n",
    "cv = pickle.load(open(\"cv.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 5\n",
      "Number of terms: 1409\n"
     ]
    }
   ],
   "source": [
    "num_documents = len(corpus)\n",
    "num_terms = len(corpus[0])  # Assuming all documents have the same length\n",
    "print(\"Number of documents:\", num_documents)\n",
    "print(\"Number of terms:\", num_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create the id2word dictionary from the corpus\n",
    "id2word = corpora.Dictionary.from_corpus(corpus, id2word=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.027*\"4145\" + 0.011*\"87\" + 0.011*\"3989\" + 0.010*\"1580\" + 0.010*\"3644\" + 0.008*\"3676\" + 0.007*\"1453\" + 0.006*\"2286\" + 0.006*\"1910\" + 0.006*\"1469\"'),\n",
       " (1,\n",
       "  '0.035*\"4145\" + 0.011*\"2329\" + 0.010*\"3644\" + 0.009*\"3989\" + 0.009*\"3676\" + 0.009*\"1984\" + 0.008*\"4150\" + 0.008*\"1910\" + 0.008*\"1587\" + 0.007*\"2369\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.030*\"2329\" + 0.029*\"4145\" + 0.023*\"1984\" + 0.013*\"3444\" + 0.012*\"2369\" + 0.010*\"2104\" + 0.008*\"1587\" + 0.008*\"3676\" + 0.008*\"3989\" + 0.008*\"3644\"'),\n",
       " (1,\n",
       "  '0.032*\"4145\" + 0.010*\"3676\" + 0.009*\"2369\" + 0.009*\"3644\" + 0.009*\"1125\" + 0.008*\"4150\" + 0.007*\"1587\" + 0.007*\"2688\" + 0.007*\"1910\" + 0.007*\"3989\"'),\n",
       " (2,\n",
       "  '0.035*\"4145\" + 0.012*\"3644\" + 0.012*\"3989\" + 0.009*\"1910\" + 0.009*\"87\" + 0.008*\"3676\" + 0.008*\"3707\" + 0.007*\"155\" + 0.007*\"4150\" + 0.007*\"1453\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"4145\" + 0.001*\"3989\" + 0.001*\"3676\" + 0.001*\"87\" + 0.001*\"3644\" + 0.001*\"4150\" + 0.001*\"1587\" + 0.001*\"2369\" + 0.001*\"155\" + 0.001*\"1453\"'),\n",
       " (1,\n",
       "  '0.034*\"2329\" + 0.030*\"4145\" + 0.026*\"1984\" + 0.015*\"3444\" + 0.012*\"2369\" + 0.011*\"2104\" + 0.009*\"1587\" + 0.008*\"3989\" + 0.008*\"2044\" + 0.008*\"3676\"'),\n",
       " (2,\n",
       "  '0.034*\"4145\" + 0.011*\"3676\" + 0.011*\"2369\" + 0.009*\"1125\" + 0.009*\"3644\" + 0.009*\"4150\" + 0.008*\"1587\" + 0.008*\"2688\" + 0.007*\"1910\" + 0.007*\"3989\"'),\n",
       " (3,\n",
       "  '0.036*\"4145\" + 0.012*\"3644\" + 0.012*\"3989\" + 0.009*\"1910\" + 0.009*\"87\" + 0.008*\"3676\" + 0.008*\"3707\" + 0.007*\"155\" + 0.007*\"4150\" + 0.007*\"1453\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns Only)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DESPICABLE ME 3</th>\n",
       "      <td>animated adventure despicable felonious gru lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEMENTAL</th>\n",
       "      <td>film journeys alongside unlikely pair ember wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLAUS</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE BOSS BABY</th>\n",
       "      <td>tim his boss baby little bro ted have become a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE MONKEY KING</th>\n",
       "      <td>inspired epic chinese tale monkey king actionp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        transcript\n",
       "DESPICABLE ME 3  animated adventure despicable felonious gru lu...\n",
       "ELEMENTAL        film journeys alongside unlikely pair ember wa...\n",
       "KLAUS                                                             \n",
       "THE BOSS BABY    tim his boss baby little bro ted have become a...\n",
       "THE MONKEY KING  inspired epic chinese tale monkey king actionp..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DESPICABLE ME 3</th>\n",
       "      <td>adventure gru lucy wilde agents avl plans chil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEMENTAL</th>\n",
       "      <td>film journeys city fire water land airresident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLAUS</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE BOSS BABY</th>\n",
       "      <td>boss baby bro adults tim dad hedge fund ceo bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE MONKEY KING</th>\n",
       "      <td>epic monkey family comedy monkey jimmy fightin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        transcript\n",
       "DESPICABLE ME 3  adventure gru lucy wilde agents avl plans chil...\n",
       "ELEMENTAL        film journeys city fire water land airresident...\n",
       "KLAUS                                                             \n",
       "THE BOSS BABY    boss baby bro adults tim dad hedge fund ceo bo...\n",
       "THE MONKEY KING  epic monkey family comedy monkey jimmy fightin..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Convert frozenset of stop words to a list\n",
    "stop_words_list = list(stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words='english')\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())\n",
    "data_dtmn.index = data_nouns.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.034*\"music\" + 0.024*\"grunts\" + 0.021*\"monkey\" + 0.011*\"king\" + 0.010*\"hey\" + 0.010*\"groans\" + 0.010*\"water\" + 0.010*\"stick\" + 0.009*\"youre\" + 0.008*\"world\"'),\n",
       " (1,\n",
       "  '0.014*\"grunts\" + 0.013*\"tim\" + 0.013*\"youre\" + 0.012*\"chuckles\" + 0.011*\"thats\" + 0.011*\"time\" + 0.009*\"tabitha\" + 0.009*\"gasps\" + 0.009*\"gru\" + 0.008*\"hey\"')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"grunts\" + 0.018*\"monkey\" + 0.017*\"music\" + 0.013*\"youre\" + 0.011*\"tim\" + 0.011*\"hey\" + 0.010*\"thats\" + 0.010*\"king\" + 0.010*\"yells\" + 0.009*\"time\"'),\n",
       " (1,\n",
       "  '0.022*\"gru\" + 0.012*\"chuckles\" + 0.012*\"gasps\" + 0.011*\"girls\" + 0.010*\"agnes\" + 0.010*\"sighs\" + 0.009*\"hey\" + 0.009*\"dru\" + 0.009*\"brother\" + 0.008*\"bratt\"'),\n",
       " (2,\n",
       "  '0.031*\"music\" + 0.021*\"grunts\" + 0.018*\"water\" + 0.015*\"chuckles\" + 0.011*\"ember\" + 0.010*\"playing\" + 0.009*\"gasps\" + 0.008*\"groans\" + 0.008*\"dad\" + 0.008*\"city\"')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"tim\" + 0.018*\"grunts\" + 0.015*\"youre\" + 0.015*\"tabitha\" + 0.014*\"thats\" + 0.012*\"time\" + 0.012*\"baby\" + 0.012*\"chuckles\" + 0.010*\"babies\" + 0.009*\"dont\"'),\n",
       " (1,\n",
       "  '0.001*\"music\" + 0.001*\"grunts\" + 0.001*\"monkey\" + 0.001*\"hey\" + 0.001*\"youre\" + 0.001*\"stick\" + 0.001*\"gasps\" + 0.001*\"groans\" + 0.001*\"laughs\" + 0.001*\"thats\"'),\n",
       " (2,\n",
       "  '0.025*\"music\" + 0.019*\"grunts\" + 0.017*\"chuckles\" + 0.013*\"gasps\" + 0.012*\"water\" + 0.012*\"gru\" + 0.009*\"youre\" + 0.009*\"hey\" + 0.009*\"playing\" + 0.008*\"sighs\"'),\n",
       " (3,\n",
       "  '0.038*\"monkey\" + 0.034*\"music\" + 0.024*\"grunts\" + 0.020*\"king\" + 0.017*\"stick\" + 0.014*\"hey\" + 0.014*\"laughs\" + 0.013*\"dragon\" + 0.013*\"world\" + 0.013*\"yells\"')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DESPICABLE ME 3</th>\n",
       "      <td>animated adventure despicable felonious gru lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEMENTAL</th>\n",
       "      <td>film journeys unlikely pair wade city fire wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLAUS</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE BOSS BABY</th>\n",
       "      <td>boss baby little bro adults other tim married ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE MONKEY KING</th>\n",
       "      <td>epic chinese tale monkey family comedy charism...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        transcript\n",
       "DESPICABLE ME 3  animated adventure despicable felonious gru lu...\n",
       "ELEMENTAL        film journeys unlikely pair wade city fire wat...\n",
       "KLAUS                                                             \n",
       "THE BOSS BABY    boss baby little bro adults other tim married ...\n",
       "THE MONKEY KING  epic chinese tale monkey family comedy charism..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aah</th>\n",
       "      <th>able</th>\n",
       "      <th>absentminded</th>\n",
       "      <th>absurd</th>\n",
       "      <th>academy</th>\n",
       "      <th>accent</th>\n",
       "      <th>accomplish</th>\n",
       "      <th>accomplishment</th>\n",
       "      <th>acorn</th>\n",
       "      <th>act</th>\n",
       "      <th>...</th>\n",
       "      <th>youryour</th>\n",
       "      <th>youve</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombielike</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>àshfá</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DESPICABLE ME 3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEMENTAL</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLAUS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE BOSS BABY</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE MONKEY KING</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2985 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 aah  able  absentminded  absurd  academy  accent  accomplish  \\\n",
       "DESPICABLE ME 3    0     1             0       0        1       0           0   \n",
       "ELEMENTAL          2     2             0       0        0       0           0   \n",
       "KLAUS              0     0             0       0        0       0           0   \n",
       "THE BOSS BABY      4     0             1       0        0       1           1   \n",
       "THE MONKEY KING    0     0             0       1        0       0           0   \n",
       "\n",
       "                 accomplishment  acorn  act  ...  youryour  youve  yum  yummy  \\\n",
       "DESPICABLE ME 3               0      0    0  ...         0      4    1      0   \n",
       "ELEMENTAL                     0      0    3  ...         0      1    0      0   \n",
       "KLAUS                         0      0    0  ...         0      0    0      0   \n",
       "THE BOSS BABY                 0      5    0  ...         1      2    0      0   \n",
       "THE MONKEY KING               2      0    1  ...         0      2    5      2   \n",
       "\n",
       "                 yup  zip  zombielike  zombies  zone  àshfá  \n",
       "DESPICABLE ME 3    0    0           0        0     0      0  \n",
       "ELEMENTAL          1    0           0        0     0      8  \n",
       "KLAUS              0    0           0        0     0      0  \n",
       "THE BOSS BABY      0    0           1        2     1      0  \n",
       "THE MONKEY KING    0    1           0        0     0      0  \n",
       "\n",
       "[5 rows x 2985 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Convert frozenset of stop words to a list\n",
    "stop_words_list = list(stop_words)\n",
    "\n",
    "# Recreate a document-term matrix using only nouns and adjectives, and remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words_list, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.052*\"monkey\" + 0.021*\"stick\" + 0.020*\"music\" + 0.015*\"grunts\" + 0.012*\"lin\" + 0.012*\"king\" + 0.010*\"hey\" + 0.009*\"laughs\" + 0.009*\"dragon\" + 0.008*\"world\"'),\n",
       " (1,\n",
       "  '0.013*\"grunts\" + 0.011*\"gasps\" + 0.010*\"chuckles\" + 0.010*\"music\" + 0.009*\"tim\" + 0.008*\"hey\" + 0.008*\"good\" + 0.007*\"gru\" + 0.006*\"tabitha\" + 0.005*\"baby\"')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"music\" + 0.015*\"grunts\" + 0.013*\"water\" + 0.012*\"ember\" + 0.011*\"gasps\" + 0.011*\"chuckles\" + 0.008*\"wade\" + 0.008*\"fire\" + 0.007*\"playing\" + 0.007*\"dad\"'),\n",
       " (1,\n",
       "  '0.022*\"gru\" + 0.015*\"minionese\" + 0.012*\"gasps\" + 0.009*\"chuckles\" + 0.009*\"dru\" + 0.008*\"hey\" + 0.008*\"girls\" + 0.008*\"bratt\" + 0.007*\"agnes\" + 0.007*\"good\"'),\n",
       " (2,\n",
       "  '0.030*\"monkey\" + 0.016*\"grunts\" + 0.012*\"stick\" + 0.012*\"music\" + 0.011*\"tim\" + 0.010*\"hey\" + 0.008*\"gasps\" + 0.008*\"tabitha\" + 0.007*\"lin\" + 0.007*\"king\"')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"grunts\" + 0.001*\"music\" + 0.001*\"gasps\" + 0.001*\"monkey\" + 0.001*\"tim\" + 0.001*\"hey\" + 0.001*\"chuckles\" + 0.001*\"baby\" + 0.001*\"whoa\" + 0.001*\"gru\"'),\n",
       " (1,\n",
       "  '0.018*\"music\" + 0.014*\"gasps\" + 0.013*\"grunts\" + 0.012*\"gru\" + 0.012*\"chuckles\" + 0.009*\"water\" + 0.008*\"minionese\" + 0.008*\"ember\" + 0.008*\"hey\" + 0.007*\"good\"'),\n",
       " (2,\n",
       "  '0.031*\"monkey\" + 0.016*\"grunts\" + 0.013*\"stick\" + 0.013*\"music\" + 0.011*\"tim\" + 0.010*\"hey\" + 0.008*\"gasps\" + 0.008*\"tabitha\" + 0.007*\"lin\" + 0.007*\"king\"'),\n",
       " (3,\n",
       "  '0.001*\"monkey\" + 0.001*\"grunts\" + 0.001*\"music\" + 0.001*\"gasps\" + 0.001*\"hey\" + 0.001*\"tim\" + 0.001*\"chuckles\" + 0.001*\"stick\" + 0.001*\"tabitha\" + 0.001*\"groans\"')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.060*\"monkey\" + 0.025*\"stick\" + 0.024*\"music\" + 0.017*\"grunts\" + 0.014*\"king\" + 0.014*\"lin\" + 0.011*\"hey\" + 0.010*\"laughs\" + 0.010*\"dragon\" + 0.009*\"world\"'),\n",
       " (1,\n",
       "  '0.018*\"music\" + 0.014*\"gasps\" + 0.013*\"grunts\" + 0.012*\"gru\" + 0.012*\"chuckles\" + 0.009*\"water\" + 0.008*\"minionese\" + 0.008*\"ember\" + 0.008*\"hey\" + 0.007*\"good\"'),\n",
       " (2,\n",
       "  '0.020*\"tim\" + 0.014*\"tabitha\" + 0.013*\"grunts\" + 0.010*\"baby\" + 0.009*\"good\" + 0.009*\"chuckles\" + 0.009*\"gasps\" + 0.008*\"tina\" + 0.008*\"hey\" + 0.008*\"babies\"'),\n",
       " (3,\n",
       "  '0.000*\"music\" + 0.000*\"grunts\" + 0.000*\"monkey\" + 0.000*\"hey\" + 0.000*\"gasps\" + 0.000*\"yells\" + 0.000*\"good\" + 0.000*\"world\" + 0.000*\"bam\" + 0.000*\"bigger\"')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: music, parents\n",
    "* Topic 1: monkey, wife\n",
    "* Topic 2: likes\n",
    "* Topic 3: gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "#list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment:\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nouns_verbs(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and verbs.'''\n",
    "    is_noun_verb = lambda pos: pos[:2] == 'NN' or pos[:2] == 'VB' or pos[:2] == 'VBG'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_verbs = [word for (word, pos) in pos_tag(tokenized) if is_noun_verb(pos)] \n",
    "    return ' '.join(nouns_verbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DESPICABLE ME 3</th>\n",
       "      <td>animated adventure despicable felonious gru lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEMENTAL</th>\n",
       "      <td>film journeys unlikely pair wade city fire wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLAUS</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE BOSS BABY</th>\n",
       "      <td>boss baby little bro adults other tim married ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE MONKEY KING</th>\n",
       "      <td>epic chinese tale monkey family comedy charism...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        transcript\n",
       "DESPICABLE ME 3  animated adventure despicable felonious gru lu...\n",
       "ELEMENTAL        film journeys unlikely pair wade city fire wat...\n",
       "KLAUS                                                             \n",
       "THE BOSS BABY    boss baby little bro adults other tim married ...\n",
       "THE MONKEY KING  epic chinese tale monkey family comedy charism..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_add = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aah</th>\n",
       "      <th>able</th>\n",
       "      <th>absentminded</th>\n",
       "      <th>absurd</th>\n",
       "      <th>academy</th>\n",
       "      <th>accent</th>\n",
       "      <th>accomplish</th>\n",
       "      <th>accomplishment</th>\n",
       "      <th>acorn</th>\n",
       "      <th>act</th>\n",
       "      <th>...</th>\n",
       "      <th>youryour</th>\n",
       "      <th>youve</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombielike</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>àshfá</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DESPICABLE ME 3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELEMENTAL</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLAUS</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE BOSS BABY</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THE MONKEY KING</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2985 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 aah  able  absentminded  absurd  academy  accent  accomplish  \\\n",
       "DESPICABLE ME 3    0     1             0       0        1       0           0   \n",
       "ELEMENTAL          2     2             0       0        0       0           0   \n",
       "KLAUS              0     0             0       0        0       0           0   \n",
       "THE BOSS BABY      4     0             1       0        0       1           1   \n",
       "THE MONKEY KING    0     0             0       1        0       0           0   \n",
       "\n",
       "                 accomplishment  acorn  act  ...  youryour  youve  yum  yummy  \\\n",
       "DESPICABLE ME 3               0      0    0  ...         0      4    1      0   \n",
       "ELEMENTAL                     0      0    3  ...         0      1    0      0   \n",
       "KLAUS                         0      0    0  ...         0      0    0      0   \n",
       "THE BOSS BABY                 0      5    0  ...         1      2    0      0   \n",
       "THE MONKEY KING               2      0    1  ...         0      2    5      2   \n",
       "\n",
       "                 yup  zip  zombielike  zombies  zone  àshfá  \n",
       "DESPICABLE ME 3    0    0           0        0     0      0  \n",
       "ELEMENTAL          1    0           0        0     0      8  \n",
       "KLAUS              0    0           0        0     0      0  \n",
       "THE BOSS BABY      0    0           1        2     1      0  \n",
       "THE MONKEY KING    0    1           0        0     0      0  \n",
       "\n",
       "[5 rows x 2985 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Convert frozenset of stop words to a list\n",
    "stop_words_list = list(stop_words)\n",
    "\n",
    "# Recreate a document-term matrix using only nouns and adjectives, and remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words_list, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_add.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n",
    "data_dtmna.index = data_nouns_add.index\n",
    "data_dtmna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"grunts\" + 0.011*\"gasps\" + 0.010*\"chuckles\" + 0.010*\"music\" + 0.009*\"tim\" + 0.008*\"hey\" + 0.008*\"good\" + 0.007*\"gru\" + 0.006*\"tabitha\" + 0.005*\"okay\"'),\n",
       " (1,\n",
       "  '0.051*\"monkey\" + 0.021*\"stick\" + 0.020*\"music\" + 0.015*\"grunts\" + 0.012*\"lin\" + 0.012*\"king\" + 0.010*\"hey\" + 0.009*\"laughs\" + 0.009*\"dragon\" + 0.008*\"world\"')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.024*\"music\" + 0.016*\"grunts\" + 0.014*\"water\" + 0.013*\"ember\" + 0.012*\"gasps\" + 0.011*\"chuckles\" + 0.008*\"fire\" + 0.008*\"wade\" + 0.008*\"playing\" + 0.007*\"dad\"'),\n",
       " (1,\n",
       "  '0.020*\"tim\" + 0.014*\"tabitha\" + 0.013*\"grunts\" + 0.010*\"baby\" + 0.009*\"chuckles\" + 0.009*\"good\" + 0.009*\"gasps\" + 0.008*\"tina\" + 0.008*\"hey\" + 0.008*\"babies\"'),\n",
       " (2,\n",
       "  '0.024*\"gru\" + 0.016*\"minionese\" + 0.012*\"gasps\" + 0.009*\"chuckles\" + 0.009*\"dru\" + 0.009*\"hey\" + 0.009*\"bratt\" + 0.009*\"girls\" + 0.008*\"agnes\" + 0.007*\"good\"'),\n",
       " (3,\n",
       "  '0.060*\"monkey\" + 0.024*\"stick\" + 0.024*\"music\" + 0.017*\"grunts\" + 0.014*\"lin\" + 0.014*\"king\" + 0.011*\"hey\" + 0.010*\"laughs\" + 0.010*\"dragon\" + 0.009*\"world\"')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
